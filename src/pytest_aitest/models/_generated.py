"""Auto-generated Pydantic models from schema/report.schema.json.

DO NOT EDIT THIS FILE DIRECTLY.
Run `python scripts/generate_models.py` to regenerate.
"""

from __future__ import annotations

from enum import StrEnum
from typing import Any, Literal

from pydantic import AwareDatetime, BaseModel, Field


class Mode(StrEnum):
    SIMPLE = "simple"
    MODEL_COMPARISON = "model_comparison"
    PROMPT_COMPARISON = "prompt_comparison"
    MATRIX = "matrix"


class TestDimensions(BaseModel):
    models: list[str]
    """
    Unique models used in tests
    """
    prompts: list[str]
    """
    Unique prompts used in tests
    """
    base_tests: list[str]
    """
    Base test names without parameters
    """
    files: list[str] | None = None
    """
    Test file paths
    """
    sessions: list[str] | None = None
    """
    Session/class names
    """


class IntStats(BaseModel):
    min: int
    max: int
    avg: int


class FloatStats(BaseModel):
    min: float
    max: float
    avg: float


class Outcome(StrEnum):
    PASSED = "passed"
    FAILED = "failed"
    SKIPPED = "skipped"


class TestMetadata(BaseModel):
    model: str | None = None
    """
    Model name if parametrized
    """
    prompt: str | None = None
    """
    Prompt name if parametrized
    """
    session: str | None = None
    """
    Session/class name
    """
    file: str | None = None
    """
    Test file path
    """


class Assertion(BaseModel):
    type: str
    """
    Assertion type (e.g., 'tool_called', 'contains')
    """
    passed: bool
    message: str | None = None
    """
    Assertion message or failure reason
    """
    details: dict[str, Any] | None = None
    """
    Additional assertion details
    """


class Role(StrEnum):
    USER = "user"
    ASSISTANT = "assistant"
    TOOL = "tool"
    SYSTEM = "system"


class ToolCall(BaseModel):
    name: str
    """
    Tool name
    """
    arguments: dict[str, Any]
    """
    Tool arguments as key-value pairs
    """
    result: str | None = None
    """
    Tool execution result
    """
    error: str | None = None
    """
    Tool execution error if any
    """
    duration_ms: float | None = Field(default=None, ge=0.0)
    """
    Tool execution duration in milliseconds
    """


class TokenUsage(BaseModel):
    prompt: int | None = Field(default=None, ge=0)
    """
    Input/prompt tokens
    """
    completion: int | None = Field(default=None, ge=0)
    """
    Output/completion tokens
    """
    total: int | None = Field(default=None, ge=0)
    """
    Total tokens (prompt + completion)
    """


class RateLimitStats(BaseModel):
    retry_count: int | None = Field(default=None, ge=0)
    """
    Number of retries due to rate limiting
    """
    total_wait_ms: float | None = Field(default=None, ge=0.0)
    """
    Total time spent waiting for rate limits
    """
    http_429_count: int | None = Field(default=None, ge=0)
    """
    Number of HTTP 429 responses received
    """


class Recommendation(BaseModel):
    configuration: str
    """
    Recommended configuration name (e.g., 'fast-agent')
    """
    summary: str
    """
    One-line recommendation
    """
    reasoning: str
    """
    Detailed explanation of why this configuration is recommended
    """
    alternatives: list[str] | None = None
    """
    Other viable options with trade-offs
    """


class FailureAnalysis(BaseModel):
    test_id: str
    """
    Full pytest node ID
    """
    configuration: str
    """
    Which configuration failed
    """
    problem: str
    """
    What went wrong (user-friendly)
    """
    root_cause: str
    """
    Why it went wrong (technical)
    """
    suggested_fix: str
    """
    How to fix it
    """


class Status(StrEnum):
    WORKING = "working"
    WARNING = "warning"
    UNUSED = "unused"
    ERROR = "error"


class ToolFeedback(BaseModel):
    tool_name: str
    status: Status
    call_count: int = Field(..., ge=0)
    error_count: int = Field(..., ge=0)
    current_description: str
    """
    The tool's current description
    """
    problem: str | None = None
    """
    What's wrong with the tool description (if anything)
    """
    suggested_description: str | None = None
    """
    Exact rewrite to use
    """
    rationale: str | None = None
    """
    Why this change helps
    """


class MCPServerFeedback(BaseModel):
    server_name: str
    tools: list[ToolFeedback]
    overall_assessment: str
    """
    Summary assessment of the MCP server
    """


class Effectiveness(StrEnum):
    EFFECTIVE = "effective"
    MIXED = "mixed"
    INEFFECTIVE = "ineffective"


class PromptFeedback(BaseModel):
    prompt_id: str
    """
    Prompt identifier (e.g., 'brief', 'detailed')
    """
    effectiveness: Effectiveness
    token_count: int = Field(..., ge=0)
    """
    Tokens in the prompt
    """
    current_excerpt: str
    """
    Relevant portion of the prompt being analyzed
    """
    problem: str | None = None
    """
    What's wrong (if anything)
    """
    suggested_change: str | None = None
    """
    Exact text to add/remove/replace
    """
    rationale: str | None = None
    """
    Why this change helps
    """


class Impact(StrEnum):
    POSITIVE = "positive"
    NEUTRAL = "neutral"
    NEGATIVE = "negative"
    UNUSED = "unused"


class SkillFeedback(BaseModel):
    skill_name: str
    usage_rate: float = Field(..., ge=0.0, le=1.0)
    """
    How often it was referenced (0-1)
    """
    impact: Impact
    token_cost: int = Field(..., ge=0)
    """
    Tokens added to prompt
    """
    current_structure: str
    """
    Brief description of skill layout
    """
    problem: str | None = None
    """
    What's wrong (if anything)
    """
    suggested_change: str | None = None
    """
    Restructuring suggestion
    """
    rationale: str | None = None
    """
    Why this change helps
    """


class Category(StrEnum):
    PROMPT = "prompt"
    SKILL = "skill"
    COST = "cost"
    EFFICIENCY = "efficiency"


class Severity(StrEnum):
    INFO = "info"
    SUGGESTION = "suggestion"
    RECOMMENDED = "recommended"


class OptimizationOpportunity(BaseModel):
    category: Category
    severity: Severity
    title: str
    current_state: str
    """
    What's happening now
    """
    suggested_change: str
    """
    What to do
    """
    expected_impact: str
    """
    What will improve
    """


class AnalysisMetadata(BaseModel):
    model: str | None = None
    """
    Model used for analysis
    """
    tokens_used: int | None = Field(default=None, ge=0)
    cost_usd: float | None = Field(default=None, ge=0.0)
    duration_ms: float | None = Field(default=None, ge=0.0)
    cached: bool | None = None
    """
    Whether the analysis was served from cache
    """


class SuiteSummary(BaseModel):
    total: int = Field(..., ge=0)
    passed: int = Field(..., ge=0)
    failed: int = Field(..., ge=0)
    skipped: int = Field(..., ge=0)
    pass_rate: float = Field(..., ge=0.0, le=100.0)
    """
    Pass rate as percentage (0-100)
    """
    total_tokens: int | None = Field(default=None, ge=0)
    total_cost_usd: float | None = Field(default=None, ge=0.0)
    total_tool_calls: int | None = Field(default=None, ge=0)
    token_stats: IntStats | None = None
    cost_stats: FloatStats | None = None
    duration_stats: FloatStats | None = None


class Turn(BaseModel):
    role: Role
    """
    Message role
    """
    content: str
    """
    Message content
    """
    tool_calls: list[ToolCall] = Field(default_factory=list)
    """
    Tool calls made in this turn (for assistant turns)
    """


class AIInsights(BaseModel):
    recommendation: Recommendation
    failures: list[FailureAnalysis]
    """
    Root cause analysis for each failed test
    """
    mcp_feedback: list[MCPServerFeedback]
    """
    Feedback on MCP server tools
    """
    prompt_feedback: list[PromptFeedback]
    """
    Feedback on system prompts
    """
    skill_feedback: list[SkillFeedback]
    """
    Feedback on agent skills
    """
    optimizations: list[OptimizationOpportunity]
    """
    Cross-cutting optimization suggestions
    """


class AgentResult(BaseModel):
    success: bool
    """
    Whether the agent completed successfully
    """
    error: str | None = None
    """
    Error message if agent failed
    """
    duration_ms: float = Field(..., ge=0.0)
    """
    Agent execution duration in milliseconds
    """
    turns: list[Turn]
    """
    Conversation turns
    """
    token_usage: TokenUsage
    cost_usd: float = Field(..., ge=0.0)
    """
    Cost in USD
    """
    final_response: str | None = None
    """
    Last assistant message content
    """
    tools_called: list[str] | None = None
    """
    List of unique tool names called
    """
    session_context_count: int | None = Field(default=None, ge=0)
    """
    Number of prior messages from session context
    """
    rate_limit_stats: RateLimitStats | None = None
    """
    Rate limiting statistics if any occurred
    """


class TestReport(BaseModel):
    name: str
    """
    Full test node ID (e.g., 'test_weather[gpt-4o-PROMPT_V1]')
    """
    outcome: Outcome
    """
    Test outcome
    """
    duration_ms: float = Field(..., ge=0.0)
    """
    Test duration in milliseconds
    """
    docstring: str | None = None
    """
    Test function's docstring for human-readable description
    """
    error: str | None = None
    """
    Error message if test failed
    """
    error_count: int | None = Field(default=None, ge=0)
    """
    Number of errors encountered (may differ from failure count)
    """
    metadata: TestMetadata | None = None
    assertions: list[Assertion] | None = None
    """
    List of assertion results
    """
    agent_result: AgentResult | None = None
    """
    Agent execution result if test used aitest_run
    """


class PytestAitestReport(BaseModel):
    schema_version: Literal["3.0"]
    """
    Schema version for compatibility checking
    """
    name: str
    """
    Name of the test suite
    """
    timestamp: AwareDatetime
    """
    ISO 8601 timestamp when report was generated
    """
    duration_ms: float = Field(..., ge=0.0)
    """
    Total duration in milliseconds
    """
    mode: Mode | None = None
    """
    Report display mode based on test dimensions
    """
    dimensions: TestDimensions | None = None
    summary: SuiteSummary
    insights: AIInsights
    """
    AI-generated analysis providing actionable insights
    """
    tests: list[TestReport]
    """
    List of all test results (evidence)
    """
    analysis_metadata: AnalysisMetadata | None = None
    """
    Metadata about the AI analysis process
    """
