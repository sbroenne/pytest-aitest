"""Auto-generated Pydantic models from schema/report.schema.json.

DO NOT EDIT THIS FILE DIRECTLY.
Run `python scripts/generate_models.py` to regenerate.
"""

from __future__ import annotations

from enum import StrEnum
from typing import Any, Literal

from pydantic import AwareDatetime, BaseModel, Field


class Mode(StrEnum):
    SIMPLE = "simple"
    MODEL_COMPARISON = "model_comparison"
    PROMPT_COMPARISON = "prompt_comparison"
    MATRIX = "matrix"


class TestDimensions(BaseModel):
    models: list[str]
    """
    Unique models used in tests
    """
    prompts: list[str]
    """
    Unique prompts used in tests
    """
    base_tests: list[str]
    """
    Base test names without parameters
    """
    files: list[str] | None = None
    """
    Test file paths
    """
    sessions: list[str] | None = None
    """
    Session/class names
    """


class IntStats(BaseModel):
    min: int
    max: int
    avg: int


class FloatStats(BaseModel):
    min: float
    max: float
    avg: float


class Outcome(StrEnum):
    PASSED = "passed"
    FAILED = "failed"
    SKIPPED = "skipped"


class TestMetadata(BaseModel):
    model: str | None = None
    """
    Model name if parametrized
    """
    prompt: str | None = None
    """
    Prompt name if parametrized
    """
    session: str | None = None
    """
    Session/class name
    """
    file: str | None = None
    """
    Test file path
    """


class Assertion(BaseModel):
    type: str
    """
    Assertion type (e.g., 'tool_called', 'contains')
    """
    passed: bool
    message: str | None = None
    """
    Assertion message or failure reason
    """
    details: dict[str, Any] | None = None
    """
    Additional assertion details
    """


class Role(StrEnum):
    USER = "user"
    ASSISTANT = "assistant"
    TOOL = "tool"
    SYSTEM = "system"


class ToolCall(BaseModel):
    name: str
    """
    Tool name
    """
    arguments: dict[str, Any]
    """
    Tool arguments as key-value pairs
    """
    result: str | None = None
    """
    Tool execution result
    """
    error: str | None = None
    """
    Tool execution error if any
    """
    duration_ms: float | None = Field(default=None, ge=0.0)
    """
    Tool execution duration in milliseconds
    """


class TokenUsage(BaseModel):
    prompt: int | None = Field(default=None, ge=0)
    """
    Input/prompt tokens
    """
    completion: int | None = Field(default=None, ge=0)
    """
    Output/completion tokens
    """
    total: int | None = Field(default=None, ge=0)
    """
    Total tokens (prompt + completion)
    """


class RateLimitStats(BaseModel):
    retry_count: int | None = Field(default=None, ge=0)
    """
    Number of retries due to rate limiting
    """
    total_wait_ms: float | None = Field(default=None, ge=0.0)
    """
    Total time spent waiting for rate limits
    """
    http_429_count: int | None = Field(default=None, ge=0)
    """
    Number of HTTP 429 responses received
    """


class SuiteSummary(BaseModel):
    total: int = Field(..., ge=0)
    passed: int = Field(..., ge=0)
    failed: int = Field(..., ge=0)
    skipped: int = Field(..., ge=0)
    pass_rate: float = Field(..., ge=0.0, le=100.0)
    """
    Pass rate as percentage (0-100)
    """
    total_tokens: int | None = Field(default=None, ge=0)
    total_cost_usd: float | None = Field(default=None, ge=0.0)
    total_tool_calls: int | None = Field(default=None, ge=0)
    token_stats: IntStats | None = None
    cost_stats: FloatStats | None = None
    duration_stats: FloatStats | None = None


class Turn(BaseModel):
    role: Role
    """
    Message role
    """
    content: str
    """
    Message content
    """
    tool_calls: list[ToolCall] = Field(default_factory=list)
    """
    Tool calls made in this turn (for assistant turns)
    """


class AgentResult(BaseModel):
    success: bool
    """
    Whether the agent completed successfully
    """
    error: str | None = None
    """
    Error message if agent failed
    """
    duration_ms: float = Field(..., ge=0.0)
    """
    Agent execution duration in milliseconds
    """
    turns: list[Turn]
    """
    Conversation turns
    """
    token_usage: TokenUsage
    cost_usd: float = Field(..., ge=0.0)
    """
    Cost in USD
    """
    final_response: str | None = None
    """
    Last assistant message content
    """
    tools_called: list[str] | None = None
    """
    List of unique tool names called
    """
    session_context_count: int | None = Field(default=None, ge=0)
    """
    Number of prior messages from session context
    """
    rate_limit_stats: RateLimitStats | None = None
    """
    Rate limiting statistics if any occurred
    """


class TestReport(BaseModel):
    name: str
    """
    Full test node ID (e.g., 'test_weather[gpt-4o-PROMPT_V1]')
    """
    outcome: Outcome
    """
    Test outcome
    """
    duration_ms: float = Field(..., ge=0.0)
    """
    Test duration in milliseconds
    """
    docstring: str | None = None
    """
    Test function's docstring for human-readable description
    """
    error: str | None = None
    """
    Error message if test failed
    """
    error_count: int | None = Field(default=None, ge=0)
    """
    Number of errors encountered (may differ from failure count)
    """
    metadata: TestMetadata | None = None
    assertions: list[Assertion] | None = None
    """
    List of assertion results
    """
    agent_result: AgentResult | None = None
    """
    Agent execution result if test used aitest_run
    """


class PytestAitestReport(BaseModel):
    schema_version: Literal["2.0"]
    """
    Schema version for compatibility checking
    """
    name: str
    """
    Name of the test suite
    """
    timestamp: AwareDatetime
    """
    ISO 8601 timestamp when report was generated
    """
    duration_ms: float = Field(..., ge=0.0)
    """
    Total duration in milliseconds
    """
    mode: Mode | None = None
    """
    Report display mode based on test dimensions
    """
    dimensions: TestDimensions | None = None
    summary: SuiteSummary
    tests: list[TestReport]
    """
    List of all test results
    """
    ai_summary: str | None = None
    """
    Optional AI-generated analysis of test results
    """
