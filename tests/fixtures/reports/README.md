# Report Fixtures

JSON fixtures for testing HTML report generation. Each fixture represents a different report scenario.

## Fixture Inventory

| Fixture | Mode | Tests | Key Features | Source |
|---------|------|-------|--------------|--------|
| `01_basic_usage.json` | simple | 12 | pass/fail, tools, docstrings | `test_basic_usage.py` |
| `02_model_comparison.json` | model_comparison | 6 | 2 models (gpt-5-mini, gpt-4.1) | `test_model_benchmark.py` |
| `03_prompt_comparison.json` | prompt_comparison | 6 | 3 prompts (brief, detailed, structured) | `test_prompt_arena.py` |
| `04_matrix.json` | matrix | 12 | 2 models × 3 prompts | `test_matrix.py` |
| `05_sessions.json` | model_comparison | 8 | Session continuity, multi-turn | `test_sessions.py` |
| `06_with_ai_summary.json` | simple | 4 | AI summary section | `test_fixture_scenarios.py::TestWithAISummary` |
| `07_with_skipped.json` | simple | 6 | Skipped tests (3 skipped) | `test_fixture_scenarios.py::TestWithSkipped` |
| `08_matrix_full.json` | matrix | 18 | All features combined | `test_fixture_scenarios.py::TestMatrixFull` |

## HTML Section Coverage

Shows which HTML sections each fixture exercises:

| Section | 01 | 02 | 03 | 04 | 05 | 06 | 07 | 08 |
|---------|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|
| Header | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ |
| Summary Cards | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ |
| Model Leaderboard | ✗ | ✓ | ✗ | ✓ | ✓ | ✗ | ✗ | ✓ |
| Prompt Table | ✗ | ✗ | ✓ | ✓ | ✗ | ✗ | ✗ | ✓ |
| Matrix Grid | ✗ | ✗ | ✗ | ✓ | ✗ | ✗ | ✗ | ✓ |
| Tool Comparison | ✗ | ✓ | ✓ | ✓ | ✓ | ✗ | ✗ | ✓ |
| Side-by-Side | ✗ | ✗ | ✗ | ✓ | ✗ | ✗ | ✗ | ✓ |
| Session Flows | ✗ | ✗ | ✗ | ✗ | ✓ | ✗ | ✗ | ✗ |
| AI Summary | ✗ | ✗ | ✗ | ✗ | ✗ | ✓ | ✗ | ✓ |
| Skipped Badge | ✗ | ✗ | ✗ | ✗ | ✗ | ✗ | ✓ | ✗ |
| Failed Badge | ✓ | ✗ | ✗ | ✗ | ✗ | ✓ | ✗ | ✗ |
| Test List | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ |
| Mermaid Diagrams | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ |

## Regenerating Fixtures

Fixtures are generated by running integration tests with specific flags.

### Using the script

```bash
# Generate all missing fixtures (06, 07, 08)
python scripts/generate_fixtures.py --all

# Generate specific fixture
python scripts/generate_fixtures.py --fixture 06
python scripts/generate_fixtures.py --fixture 07
python scripts/generate_fixtures.py --fixture 08
```

### Manual generation

```bash
# Fixture 06: With AI summary
pytest tests/integration/test_fixture_scenarios.py::TestWithAISummary -v \
    --aitest-json=tests/fixtures/reports/06_with_ai_summary.json \
    --aitest-summary --aitest-summary-model=azure/gpt-5-mini

# Fixture 07: With skipped tests
pytest tests/integration/test_fixture_scenarios.py::TestWithSkipped -v \
    --aitest-json=tests/fixtures/reports/07_with_skipped.json

# Fixture 08: Matrix with all features
pytest tests/integration/test_fixture_scenarios.py::TestMatrixFull -v \
    --aitest-json=tests/fixtures/reports/08_matrix_full.json \
    --aitest-summary --aitest-summary-model=azure/gpt-5-mini
```

### Regenerating existing fixtures (01-05)

```bash
# Fixture 01: Basic usage
pytest tests/integration/test_basic_usage.py -v \
    --aitest-json=tests/fixtures/reports/01_basic_usage.json

# Fixture 02: Model comparison  
pytest tests/integration/test_model_benchmark.py -v \
    --aitest-json=tests/fixtures/reports/02_model_comparison.json

# Fixture 03: Prompt comparison
pytest tests/integration/test_prompt_arena.py -v \
    --aitest-json=tests/fixtures/reports/03_prompt_comparison.json

# Fixture 04: Matrix
pytest tests/integration/test_matrix.py -v \
    --aitest-json=tests/fixtures/reports/04_matrix.json

# Fixture 05: Sessions
pytest tests/integration/test_sessions.py -v \
    --aitest-json=tests/fixtures/reports/05_sessions.json
```

## Generating HTML for visual review

After generating/updating fixtures, generate HTML to visually verify:

```bash
python scripts/generate_fixture_html.py
```

This creates HTML files in `docs/reports/` (tracked in git for public viewing).
